{
  "exercises": [
    {
      "id": "0Y7x",
      "language": "python",
      "exercise": "import json\nimport numpy as np\nimport pandas as pd\nfrom scipy.stats import norm\n\nseed = #< SEED >#\nid = '#< ID >#'\n\nif seed is None:\n    ui32 = np.iinfo(np.uint32)\n    seed = np.random.randint(1, ui32.max)\n\nnp.random.seed(seed)\n#< #exercise >#\nX = np.round(norm.rvs(loc=1, scale=0.1), 2)\n\nex = \"Write a Python program, named $\\\\texttt{{normal.py}}$, that accepts one command line argument as an $\\\\texttt{{int}}$ and prints the answer to the following prompt.  Assume $X \\\\sim \\\\text{{Normal}}(1, 0.1)$.  Find $P(X > {0})$.\".format(X)\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'context': ex,\n                     'questions': [],\n                     'random': {'X': X}})\nprint(output)\n#< /exercise >#\n#< #solution >#\nX = np.round(norm.rvs(loc=1, scale=0.1), 2)\n\nsol = np.round(1 - norm.cdf(X, loc=1, scale=0.1), 2)\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'random': {'X': X},\n                     'solutions': sol})\nprint(output)\n#< /solution >#\n",
      "tags": ["probability", "normal", "standard normal", "right tail"]
    },
    {
      "id": "exd3",
      "language": "python",
      "exercise": "import json\n\nseed = #< SEED >#\nid = '#< ID >#'\n\n#< #exercise >#\ncntxt = \"The majority of this class focused on Normal linear models.\"\nqsts = [\"What about them is Normal?\",\n        \"What about them is linear?  For full points, you must be very specific about what exactly is linear.\",\n        \"Is it possible for normal linear models to fit curves to data?  Explain.\"]\noutput = json.dumps({\n  'seed': seed,\n  'id': id,\n  'context': cntxt,\n  'random': {},\n  'questions': qsts})\nprint(output)\n#< /exercise >#\n",
      "tags": ["normal", "linear model"]
    },
    {
      "id": "OqHz",
      "language": "r",
      "exercise": "suppressPackageStartupMessages(library(stats))\nsuppressPackageStartupMessages(library(jsonlite))\nsuppressPackageStartupMessages(library(pander))\nsuppressPackageStartupMessages(library(dplyr))\n\ntree <- read.csv(\"https://vincentarelbundock.github.io/Rdatasets/csv/datasets/trees.csv\") %>%\n  mutate(g = Girth,\n         h = Height,\n         v = Volume,\n         p = Girth*Height,\n         type=factor(rep(LETTERS[1:2], length.out=31)))\n\nseed <- #< SEED >#\nid <- \"#< ID >#\"\n\nif (is.null(seed)) {\n    seed <- sample(.Machine$integer.max, 1)\n}\n\nset.seed(seed)\n\nN <- nrow(tree)\ntree <- tree[sample(N, N, replace=TRUE), ]\n\nfit <- tree %>%\n  lm(v ~ g*h + type, data=.)\n\nbeta <- fit %>%\n  .[[1]] %>%\n  round(2)\n\nbeta_table <- beta %>%\n  pandoc.table.return %>%\n  pandoc.indent(1)\n\nconf <- fit %>%\n  confint(level=.92) %>%\n  round(2)\n\nconf_table <- conf %>%\n  pandoc.table.return %>%\n  pandoc.indent(1)\n\n#< #exercise >#\ncontext <- \"The dataset trees consists of 31 observations on two types of trees, $A$ and $B$.  For each tree, height (h, feet), girth (g, diameter in inches), and volume (v, feet$^3$) were measured.\\n\\n\\tEstimated coefficients and confidence intervals appear below.\"\n\nquestions <- c(\"Identify the response variable(s) and its(their) statistical type(s).\",\n               \"Identify the explanatory variable(s) and its(their) statistical type(s).\",\n               \"Provide R code to reproduce the plot above.\",\n               \"Given the output above, write 1 complete English sentence describing the estimated intercept for type A trees.\",\n               \"Does the estimated intercept for type A trees make sense in context of these data.  Explain why or why not.\",\n               \"Given the output above, write 1 complete English sentence describing the estimated intercept for type B trees.\",\n               \"Does the estimated intercept for type B trees make sense in context of these data.  Explain why or why not.\",\n               \"Note that the fit model has an interaction term between two numerical explanatory variables.  This is new.  Why does an interaction term between Height and Girth (diameter) make sense in the context of this predictive model?  Hint: think geometrically.  Explain.\",\n               \"Given the output above, write 1 complete English sentence describing the estimated slope across Height}.  State clearly to which type(s) of trees this slope applies.  Be careful with your derivative.\",\n               \"Given the output above, write 1 complete English sentence describing the estimated slope across Girth.  State clearly to which type(s) of trees this slope applies.  Be careful with your derivative.\",\n               \"Provide R code to calculate the mean of Girth and the mean of Height by levels of the categorical variable type.  If you use any library, be sure to load it.\",\n               \"Write down either R code or mathematical symbols that would make a prediction for the Volume of a type B tree when Height is equal to its mean, call it $\\\\bar{H}$, and when Girth is equal to its mean, call it $\\\\bar{G}$.\",\n               \"Assume your code above predicts the number $28.44$. Interpret this number in context of these data.\",\n               \"Write down either R code or mathematical symbols that would make a prediction for the Volume of a type B tree when Height is equal to its mean, call it $\\\\bar{H}$, and when Girth is equal to $40$.\",\n               \"Using words from our class, which prediction is more reasonable at Girth equal to $\\\\bar{G}$ or at Girth equal to $40$?  Why?\",\n               \"Interpret the confidence interval for the term factor(type)B in context of these data.\",\n               \"Do these data suggest a significant difference between type A and type B trees?  Explain.\")\n\noutput <- toJSON(list(\n  id = id,\n  seed = seed,\n  context = paste0(context, beta_table, conf_table),\n  random = list(beta=beta, conf=conf),\n  questions = questions))\n\ncat(output)\n#< /exercise >#\n",
      "tags": [
        "linear model",
        "regression",
        "confidence interval",
        "interpret",
        "interaction",
        "extrapoloation"
      ]
    },
    {
      "id": "pOkn",
      "language": "r",
      "exercise": "suppressPackageStartupMessages(library(jsonlite))\n\nseed <- #< SEED >#\nid <- '#< ID >#'\n\noutput <- toJSON(list(\n  id = id,\n  seed = seed,\n  context = \"The minority of this class focused on Logistic regression.  What are the two main differences between logistic and linear regression?\",\n  questions = c(),\n  random = list()))\n\ncat(output)\n",
      "tags": ["logistic regression", "linear model", "regression"]
    },
    {
      "id": "aqdK",
      "language": "r",
      "exercise": "suppressPackageStartupMessages(library(jsonlite))\n\nseed <- #< SEED >#\nid <- '#< ID >#'\n\nadmit <- read.csv(\"https://raw.githubusercontent.com/roualdes/data/master/admissions.csv\")\n\nX <- model.matrix(~ 0 + gre + gpa, data=admit)\n\npred_logistic <- function(mX, betahat) {\n  lin <- apply(mX, 1, function(row) {sum(betahat * row)})\n  1 / (1 + exp(-lin))\n}\n\nll <- function(beta, y, mX) {\n    beta0 <- beta[1]\n    betas <- beta[-1]\n    lin <- apply(mX, 1, function(row) {beta0 + sum(betas * row)})\n    sum( log1p(exp(lin)) - y*lin )\n}\n\nbeta_hat <- optim(rnorm(3), ll, method=\"L-BFGS-B\", y=admit$admit, mX=scale(X))$par\n\nphat <- round(pred_logistic(matrix(c(1, 0, 0), ncol=3), beta_hat), 2)\n\nblogistic <- function(data, idx) {\n    y <- data[idx, 1]\n    X <- data[idx, -1]\n    beta_hat <- optim(rnorm(3), ll, method=\"L-BFGS-B\", y=y, mX=X)$par\n    diff(pred_logistic(matrix(c(1, 0, 0, # intercept, gre, gpa\n                                1, 0, 1),\n                              ncol=3, byrow=TRUE), beta_hat))\n}\n\nb <- boot::boot(cbind(admit$admit, scale(X)),\n                R=999,\n                blogistic,\n                ncpus=3, parallel=\"multicore\")\nseed <- b$seed\nci <- round(boot::boot.ci(b, type=\"perc\")$percent[4:5], 2)\n\n#< #exercise >#\ncontext <- \"The dataset $\\\\texttt{admission}$ contains 400 randomly selected\n  students' $\\\\texttt{gpa}$, $\\\\texttt{gre}$ (graduate school equivalent of\n  SAT) scores, and a Bernoulli random variable named $\\\\texttt{admit}$\n  that takes on the value $1$ if the student was admitted into\n  graduate school and a $0$ otherwise.\\n\\n\\tAssume you fit with logistic regression the response variable $\\\\texttt{admit}$ to the $\\\\textit{normalized}$ numerical explanatory variables $\\\\texttt{gre}$ and $\\\\texttt{gpa}$, in this order.\"\n\nquestions <- c(sprintf(\"With a row of the model matrix as \\`c(1, 0, 0)\\`, interpret, in context of the data, the predicted probability p = %.2f.\", phat),\n               paste(\"With a row of the model matrix as \\`c(1, 0, 0)\\`, interpret, in context of the data, the 95% confidence interval\",  sprintf(\"(%.2f, %.2f).\", ci[1], ci[2])))\n\noutput <- toJSON(list(\n  id = id,\n  seed = seed,\n  context = context,\n  questions = questions,\n  random = list(phat = phat, ci = ci)))\ncat(output)\n#< /exercise >#\n",
      "tags": [
        "logistic regression",
        "linear model",
        "regression",
        "confidence interval",
        "interpret",
        "predict"
      ]
    },
    {
      "id": "ey0C",
      "language": "python",
      "exercise": "import json\nimport numpy as np\nfrom scipy.stats import norm\n\nseed = #< SEED >#\nid = '#< ID >#'\n\nnpint = np.iinfo(np.int)\nif seed is None:\n    seed = np.random.randint(1, npint.max)\nnp.random.seed(seed)\n\nb = int(np.random.randint(1, 314, 1)[0])\n\n#< #exercise >#\nex = \"Assume $X \\\\sim \\\\text{{Uniform}}(a = 1, b = {0})$.  Calculate $P(X = 1)$, rounding to $4$ decimal places.  The Python script $\\\\texttt{{uniform.py}}$ is provided to help you start.  Note that the upper bound of the support $b$ will be randomly chosen based on the $\\\\text{{seed}}$.  Your script needs to provide the correct answer relative to the randomly chosen value $b$.\".format(b)\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'context': ex,\n                     'questions': [],\n                     'random': {'b': b}})\n\nprint(output)\n#< /exercise >#\n#< #solution >#\nsol = np.round(1 / b, 4)\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'random': {'b': b},\n                     'solutions': [sol]})\nprint(output)\n#< /solution >#\n",
      "tags": ["probability", "uniform", "discrete"]
    },
    {
      "id": "JoUe",
      "language": "python",
      "exercise": "import json\nimport numpy as np\nfrom scipy.stats import norm\n\nseed = #< SEED >#\nid = '#< ID >#'\n\nnpint = np.iinfo(np.int)\nif seed is None:\n    seed = np.random.randint(1, npint.max)\nnp.random.seed(seed)\n\nb = int(np.random.randint(1, 314, 1)[0])\n\n#< #exercise >#\nex = \"Assume $X \\\\sim \\\\text{{Uniform}}(a=1, b={0})$. Estimate $P(X=1)$.  Use $N = 10,000$ samples and round to $4$ decimal places. The Python script $\\\\texttt{{est_𝚞𝚗𝚒𝚏𝚘𝚛𝚖.𝚙𝚢}}$ is provided to help you start. Note that the upper bound of the support $b$ will be randomly chosen based on the seed. Your script needs to provide the correct answer relative to the randomly chosen value b.\".format(b)\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'context': ex,\n                     'questions': [],\n                     'random': {'b': b}})\n\nprint(output)\n#< /exercise >#\n#< #solution >#\nx = np.random.choice(b + 1, int(1e4))\nsol = np.round(np.mean(x == 1), 4)\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'random': {'b': b},\n                     'solutions': [sol]})\nprint(output)\n#< /solution >#\n",
      "tags": ["probability", "uniform", "discrete", "simulate", "python"]
    },
    {
      "id": "pKsd",
      "language": "python",
      "exercise": "import json\nimport numpy as np\nfrom scipy.stats import norm\n\nseed = #< SEED >#\nid = '#< ID >#'\n\nnpint = np.iinfo(np.int)\nif seed is None:\n    seed = np.random.randint(1, npint.max)\nnp.random.seed(seed)\n\nX = np.random.binomial(1, 0.314, int(1e4))\n\n#< #exercise >#\nex = \"Write a Python program, named $\\\\texttt{ll_bernoulli.py}$, that accepts one command line argument as an $\\\\texttt{int}$ and prints the answer to the following prompt.  With the data $X_n \\\\sim \\\\text{Bernoulli}(p)$ for $n = 1, \\\\ldots, N$ evaluate the log-likelihood function for the Bernoulli distribution at each of the points $p \\\\in \\\\{{0.1, 0.2, \\ldots, 0.9\\\\}}$.  Round to 4 decimal places.  The function $\\\\texttt{numpy.linspace()}$ could be used to generate the array of points $p$.\"\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'context': ex,\n                     'questions': [],\n                     'random': {'X': X.tolist()}})\n\nprint(output)\n#< /exercise >#\n#< #solution >#\nSX = np.sum(X)\np = np.linspace(0.1, 0.9, 9)\nll = np.log(p) * SX + np.log(1- p) * (X.size - SX)\nsol = np.round(ll, 4)\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'random': {'X': X.tolist()},\n                     'solutions': sol.tolist()})\nprint(output)\n#< /solution >#\n",
      "tags": ["log-likelihood", "bernoulli", "discrete"]
    },
    {
      "id": "ENId",
      "language": "python",
      "exercise": "import json\nimport numpy as np\nfrom scipy.stats import norm\n\nseed = #< SEED >#\nid = '#< ID >#'\n\nnpint = np.iinfo(np.int)\nif seed is None:\n    seed = np.random.randint(1, npint.max)\nnp.random.seed(seed)\n\nX = np.random.binomial(1, 0.314, int(1e4))\n\n#< #exercise >#\nex = \"Write a Python program, named $\\\\texttt{dpll_bernoulli.py}$, that accepts one command line argument as an $\\\\texttt{int}$ and prints the answer to the following prompt.  With the data $X_n \\\\sim \\\\text{Bernoulli}(p)$ for $n = 1, \\\\ldots, N$ evaluate the derivative with respecct to $p$ of the log-likelihood function for the Bernoulli distribution at each of the points $p \\\\in \\\\{{0.1, 0.2, \\ldots, 0.9\\\\}}$.  Round to 4 decimal places.  The function $\\\\texttt{numpy.linspace()}$ could be used to generate the array of points $p$.\"\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'context': ex,\n                     'questions': [],\n                     'random': {'X': X.tolist()}})\n\nprint(output)\n#< /exercise >#\n#< #solution >#\nSX = np.sum(X)\np = np.linspace(0.1, 0.9, 9)\nll = SX / p - (X.size - SX) / (1 - p)\nsol = np.round(ll, 4)\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'random': {'X': X.tolist()},\n                     'solutions': sol.tolist()})\nprint(output)\n#< /solution >#\n",
      "tags": ["score function", "log-likelihood", "bernoulli", "discrete"]
    },
    {
      "id": "XuJ5",
      "language": "python",
      "exercise": "import json\nimport numpy as np\n\nseed = #< SEED >#\nid = '#< ID >#'\n\nnpint = np.iinfo(np.int)\nif seed is None:\n    seed = np.random.randint(1, npint.max)\nnp.random.seed(seed)\n\n#< #exercise >#\nex = \"Assume $X \\\\sim \\\\text{Bernoulli}(p=0.686)$. Estimate $P(X=0)$.  Use $N = 10001$ samples and round to $3$ decimal places. The Python script $\\\\texttt{est_bernoulli.𝚙𝚢}$ is provided to help you start.\"\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'context': ex,\n                     'questions': [],\n                     'random': {}})\n\nprint(output)\n#< /exercise >#\n#< #solution >#\nx = np.random.binomial(1, 0.314, 10001)\nsol = np.round(np.mean(x), 3)\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'random': {},\n                     'solutions': [sol]})\nprint(output)\n#< /solution >#\n",
      "tags": [
        "probability",
        "bernoulli",
        "binomial",
        "discrete",
        "simulate",
        "python"
      ]
    },
    {
      "id": "BK7f",
      "language": "python",
      "exercise": "import json\nimport numpy as np\nfrom scipy.stats import norm\n\nseed = #< SEED >#\nid = '#< ID >#'\n\nnpint = np.iinfo(np.int)\nif seed is None:\n    seed = np.random.randint(1, npint.max)\nnp.random.seed(seed)\n\n#< #exercise >#\nex = \"Please bring your code for this assignment to class on Wednesday 2019-09-04.  There will be an open notes quiz based on this material.\"\nq01 = \"\"\"Read\n    a. [Basic Mathematical Operations Using Arrays](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html#Basic-Mathematical-Operations-Using-Arrays),\n    b. [Vectorized Operations](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html#Vectorized-Operations), and\n    c. [Logical Operations](https://www.pythonlikeyoumeanit.com/Module3_IntroducingNumpy/VectorizedOperations.html#Logical-Operations).\"\"\"\n\nq02 = \"\"\"Using the [%timeit magic](https://ipython.readthedocs.io/en/stable/interactive/magics.html#magic-timeit) and the array `x`, time how long it takes to sum all the numbers in the array `x`.  You should compare the time of two different strategies.  The first strategy should use the function `np.sum()`.  The second strategy, in a separate code cell, should use a for loop to sum all the numbers in the array.  Our syllabus has a good Python reference for you, if you want more information on `for` loops.  Here's code to produce an array `x`.\n\n```py\nx = np.random.normal(size=10001)\n```\"\"\"\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'context': ex,\n                     'questions': [q01, q02],\n                     'random': {}})\n\nprint(output)\n#< /exercise >#\n#< #solution >#\nx = np.random.binomial(1, 0.314, 10001)\nsol = np.round(np.mean(x), 3)\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'random': {},\n                     'solutions': [sol]})\nprint(output)\n#< /solution >#\n",
      "tags": ["vectorization"]
    },
    {
      "id": "zPoY",
      "language": "python",
      "exercise": "import json\nimport numpy as np\n\nseed = #< SEED >#\nid = '#< ID >#'\n\nnpint = np.iinfo(np.int)\nif seed is None:\n    seed = np.random.randint(1, npint.max)\nnp.random.seed(seed)\n\nb = int(np.random.randint(1, 314, 1)[0])\n\n#< #exercise >#\nex = \"Assume $X \\\\sim \\\\text{{Uniform}}(a=1, b={0})$. Estimate $P(X=1)$.  Explain your code in one complete English sentence\".format(b)\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'context': ex,\n                     'questions': [],\n                     'random': {'b': b}})\n\nprint(output)\n#< /exercise >#\n#< #solution >#\nx = np.random.choice(b + 1, int(1e4))\nsol = np.round(np.mean(x == 1), 4)\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'random': {'b': b},\n                     'solutions': float(sol)})\n\nprint(output)\n#< /solution >#\n",
      "tags": ["probability", "uniform", "discrete", "simulate"]
    },
    {
      "id": "jmMA",
      "language": "python",
      "exercise": "import json\nimport numpy as np\n\nseed = #< SEED >#\nid = '#< ID >#'\n\nnpint = np.iinfo(np.int)\nif seed is None:\n    seed = np.random.randint(1, npint.max)\nnp.random.seed(seed)\n\np = np.round(np.random.uniform(low=0.314, high=0.686), 2)\n\n#< #exercise >#\nex = \"Assume $X \\\\sim \\\\text{{Bernoulli}}(p={0})$. Estimate $P(X=0)$.  Explain your code in one complete English sentence.\".format(p)\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'context': ex,\n                     'questions': [],\n                     'random': {}})\n\nprint(output)\n#< /exercise >#\n#< #solution >#\nnp.random.seed(seed)\nx = np.random.binomial(1, p, 10001)\nsol = np.round(np.mean(x), 3)\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'random': {},\n                     'solutions': float(sol)})\nprint(output)\n#< /solution >#\n",
      "tags": ["probability", "bernoulli", "binomial", "discrete", "simulate"]
    },
    {
      "id": "UkX7",
      "language": "python",
      "exercise": "import json\nimport numpy as np\nimport math\n\nseed = #< SEED >#\nid = '#< ID >#'\n\nnpint = np.iinfo(np.int)\nif seed is None:\n    seed = np.random.randint(1, npint.max)\nnp.random.seed(seed)\n\n\n#< #exercise >#\nex = \"Estimate the probability of the 5 card poker hand one pair.\"\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'context': ex,\n                     'questions': [],\n                     'random': {}})\n\nprint(output)\n#< /exercise >#\n#< #solution >#\ndef choose(N, k):\n  return math.factorial(N) // (math.factorial(k) * math.factorial(N - k))\n\ns = choose(13, 1) * choose(4, 2) * choose(12, 3) * choose(4, 1)**3 / choose(52, 5)\nsol = np.round(s, 2)\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'random': {},\n                     'solutions': float(sol)})\nprint(output)\n#< /solution >#\n",
      "tags": [
        "probability",
        "bernoulli",
        "cards",
        "poker",
        "one pair",
        "simulate"
      ]
    },
    {
      "id": "UowZ",
      "language": "python",
      "exercise": "import json\nimport numpy as np\nimport math\n\nseed = #< SEED >#\nid = '#< ID >#'\n\nnpint = np.iinfo(np.int)\nif seed is None:\n    seed = np.random.randint(1, npint.max)\nnp.random.seed(seed)\n\n\n#< #exercise >#\nex = \"Estimate the probability of the 5 card poker hand two pair.\"\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'context': ex,\n                     'questions': [],\n                     'random': {}})\n\nprint(output)\n#< /exercise >#\n#< #solution >#\ndef choose(N, k):\n  return math.factorial(N) // (math.factorial(k) * math.factorial(N - k))\n\ns = choose(13, 2) * choose(4, 2)**2 * choose(12, 1) * choose(4, 1) / choose(52, 5)\nsol = np.round(s, 2)\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'random': {},\n                     'solutions': float(sol)})\nprint(output)\n#< /solution >#\n",
      "tags": [
        "probability",
        "bernoulli",
        "cards",
        "poker",
        "two pair",
        "simulate"
      ]
    },
    {
      "id": "JP3y",
      "language": "python",
      "exercise": "import json\nimport numpy as np\n\nseed = #< SEED >#\nid = '#< ID >#'\n\nnpint = np.iinfo(np.int)\nif seed is None:\n    seed = np.random.randint(1, npint.max)\nnp.random.seed(seed)\n\nX = np.random.choice([1, 2], size=2).sum()\n\n#< #exercise >#\ncntxt = f\"Define $A$ to be the event that draw number {X} from a fair deck of cards with out replacement is an Ace.  Note that the first {X - 1} draws can be anything.\"\n\nq1 = \"Calculate the true probability of event $A$.\"\nq2 = \"Estimate the probability of event $A$.\"\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'context': cntxt,\n                     'questions': [q1, q2],\n                     'random': {\"X\": int(X)}})\n\nprint(output)\n#< /exercise >#\n#< #solution >#\n\ndeck = [s + r for r in \"A23456789TJQK\" for s in \"HDCS\"]\n\nsol1 = {2: (48/52) * (4/51) + (4/52) * (3/51),\n        3: (48/52) * (47/51) * (4/50) + (48/52) * (4/51) * (3/50) + (4/52) * (3/51) * (2/50) + (4/52) * (48/51) * (3/50),\n        4: ((48 * 47 * 46 * 4) + (48 * 47 * 4 * 3) + (48 * 4 * 47 * 3) + (48 * 4 * 3 * 2) + (4 * 48 * 47 * 3) + (4 * 48 * 3 * 2) + (4 * 3 * 48 * 2) + (4 * 3 * 2 * 1)) / (52 * 51 * 50 * 49)}\n\nN = 10001\ncount = 0\nnp.random.seed(seed)\nfor n in range(N):\n  x = np.random.choice(deck, size=X, replace=False)\n  rank = x[X - 1][1]\n  if rank == \"A\":\n    count += 1\n\nsol2 = np.round(count / N, 2)\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'random': {\"X\": int(X)},\n                     'solutions': [float(np.round(sol1[X])), float(sol2)]})\nprint(output)\n#< /solution >#\n",
      "tags": ["probability", "cards", "poker", "simulate", "python"]
    },
    {
      "id": "AtwM",
      "language": "python",
      "exercise": "import json\nimport numpy as np\nfrom scipy import stats\n\nseed = #< SEED >#\nid = '#< ID >#'\n\nnpint = np.iinfo(np.int)\nif seed is None:\n    seed = np.random.randint(1, npint.max)\nnp.random.seed(seed)\n\nK = np.random.choice(np.arange(5, 16))\np = np.round(np.random.uniform(0.5, 0.9), 2)\n\n#< #exercise >#\ncntxt = f\"Let $X \\\\sim \\\\text{{Binomial}}(K = {K}, p = {p})$.  For all parts about estimation, use one array of random variables, which you can create with the function $\\\\texttt{{np.random.binomial(K, p, N)}}$.  For all parts about calculation, evaluate the probability density function of $X$ in Python with the function $\\\\texttt{{scipy.stats.binom(K, p).pmf(x)}}$.  Round all numeric answers to 2 decimal places.\"\n\n\nq1 = \"Calculate the population mean.\"\nq2 = \"Estimate the population mean.\"\nq3 = \"Calculate the population standard deviation.\"\nq4 = \"Estimate the population standard deviation using the function $\\\\texttt{np.std()}$.\"\nq5 = \"Calculate $f(x)$ for all values of $x$ in the support.\"\nq6 = \"Estimate $f(x)$ for all values of $x$ in the support.\"\nq7 = \"Plot the true population probabilites associated with each value in the support.  Use $\\\\texttt{bp.point()}$.\"\nq8 = \"Use $\\\\texttt{bp.point()}$ again to plot the esimated probabilities associated with each value in the support.  Overlay the estimates on top of the true population probabilities.  To make the estimates visually different than the population probabilities, change the style, color, and/or size of the points.\"\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'context': cntxt,\n                     'questions': [q1, q2, q3, q4, q5, q6, q7, q8],\n                     'random': {\"K\": int(K), \"p\": float(p)}})\n\nprint(output)\n#< /exercise >#\n#< #solution >#\nmu = np.round(K * p, 2)\nnp.random.seed(seed)\n\nX = np.random.binomial(K, p, 1001)\nmu_hat = np.round(X.mean(), 2)\n\nbinomial = stats.binom(K, p)\nS = np.arange(K+1)\nfx = binomial.pmf(S)\n\nfx_hat = np.asarray([(X == x).mean() for x in S])\n\nsol56 = \"\"\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'random': {\"K\": int(K), \"p\": float(p)},\n                     'solutions': [float(mu), float(mu_hat), fx.tolist(), fx_hat.tolist(), sol56, sol56]})\nprint(output)\n#< /solution >#\n",
      "tags": ["probability", "binomial", "plot", "mean", "simulate", "python"]
    },
    {
      "id": "VB66",
      "language": "python",
      "exercise": "import json\nimport numpy as np\nfrom scipy import stats\nfrom scipy.optimize import minimize\n\nseed = #< SEED >#\nid = '#< ID >#'\n\nnpint = np.iinfo(np.int)\nif seed is None:\n    seed = np.random.randint(1, npint.max)\nnp.random.seed(seed)\n\np = np.round(np.random.uniform(0.1, 0.9), 2)\n\n#< #exercise >#\ncntxt_a = f\"Let $X_1, \\\\ldots, X_N \\\\sim \\\\text{{Bernoulli}}(p = {p})$.\"\ncntxt_b = \"  The probability density function for the Bernoulli random variable is \\\\[ f(x | p) = p^x (1 - p)^{1-x}\\\\]\"\ncntxt = cntxt_a + cntxt_b\n\nq1 = \"By hand, calculate the simplified log-likelihood and then write the simplified log-likelihood as a Python function with signature $\\\\texttt{ll_bernoulli(p, x)}$.\"\nq2 = \"By  hand, calculate the maximum likelihood estimator.  Include a photo of your hand written solution, or type out your solution using LaTeX.\"\nq3 = f\"Create an array $\\\\texttt{{X}}$ by generating $N = 1001$ Bernoulli random variables, with $p = {p}$ from above.\"\nq4 = \"Use $\\\\texttt{scipy.optimize.minimize(ll, (p0), args=(X), method='L-BFGS-B', bounds=[...])}$ to calculate the maximum likelihood estimate $\\\\hat{p}$ of $p$.\"\n\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'context': cntxt,\n                     'questions': [q1, q2, q3, q4],\n                     'random': {\"p\": float(p)}})\n\nprint(output)\n#< /exercise >#\n#< #solution >#\n\nnp.random.seed(seed)\nX = np.random.binomial(1, p, 1001)\n\ndef ll_binomial(theta, x):\n    N = x.size\n    Sx = np.sum(x)\n    return -np.log(theta) * Sx - np.log(1-theta) * (N - Sx)\n\nsol = np.round(minimize(ll_binomial, (0.5), args=(X), method=\"L-BFGS-B\", bounds=[(1e-5, 1 - 1e-5)])[\"x\"], 2)\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'random': {\"p\": float(p)},\n                     'solutions': [float(sol)]})\nprint(output)\n#< /solution >#\n",
      "tags": ["likelihood", "bernoulli", "simulate", "python"]
    },
    {
      "id": "fvzV",
      "language": "python",
      "exercise": "import json\nimport numpy as np\nfrom scipy import stats\nfrom scipy.optimize import minimize\n\nseed = #< SEED >#\nid = '#< ID >#'\n\nnpint = np.iinfo(np.int)\nif seed is None:\n    seed = np.random.randint(1, npint.max)\nnp.random.seed(seed)\n\nl = np.round(np.random.uniform(1, 15), 2)\n\n#< #exercise >#\ncntxt_a = f\"Let $X_1, \\\\ldots, X_N \\\\sim \\\\text{{Exponential}}(\\\\lambda = {l})$.\"\ncntxt_b = \"  The probability density function for the Exponential random variable is \\\\[ f(x | \\\\lambda) = \\\\lambda e^{-\\\\lambda x}\\\\] for $x \\\\in (0, \\infty)$ and $\\\\lambda > 0$.\"\ncntxt = cntxt_a + cntxt_b\n\nq1 = \"By hand, calculate the simplified log-likelihood and then write the simplified log-likelihood as a Python function with signature $\\\\texttt{ll_exponential(l, x)}$.\"\nq2 = \"By  hand, calculate the maximum likelihood estimator.  Include a photo of your hand written solution, or type out your solution using LaTeX.\"\nq3 = f\"Create an array $\\\\texttt{{X}}$ by generating $N = 1001$ Exponential random variables, with $\\\\lambda = {l}$ from above.  Use $\\\\texttt{{np.random.exponential(}}\\\\lambda\\\\texttt{{, N)}}$.\"\nq4 = \"Use $\\\\texttt{scipy.optimize.minimize(ll, (l0), args=(X), method='L-BFGS-B', bounds=[...])}$ to calculate the maximum likelihood estimate $\\\\hat{\\\\lambda}$ of $\\\\lambda$.\"\n\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'context': cntxt,\n                     'questions': [q1, q2, q3, q4],\n                     'random': {\"l\": float(l)}})\n\nprint(output)\n#< /exercise >#\n#< #solution >#\n\nnp.random.seed(seed)\nX = np.random.exponential(l, 1001)\n\ndef ll_exponential(theta, x):\n    N = x.size\n    return -N * np.log(theta) + theta * np.sum(x)\n\nsol = np.round(minimize(ll_exponential, (2), args=(X), method=\"L-BFGS-B\", bounds=[(1e-5, np.inf)])[\"x\"], 2)\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'random': {\"l\": float(l)},\n                     'solutions': [float(sol)]})\nprint(output)\n#< /solution >#\n",
      "tags": ["likelihood", "exponential", "simulate", "python"]
    },
    {
      "id": "JTUp",
      "language": "python",
      "exercise": "import json\nimport numpy as np\nfrom scipy import stats\nfrom scipy.optimize import minimize\n\nseed = #< SEED >#\nid = '#< ID >#'\n\nnpint = np.iinfo(np.int)\nif seed is None:\n    seed = np.random.randint(1, npint.max)\nnp.random.seed(seed)\n\nmu = np.round(np.random.uniform(1, 15), 2)\nsigma = np.round(np.random.uniform(1, 5), 2)\n\n#< #exercise >#\ncntxt_a = f\"Let $X_1, \\\\ldots, X_N \\\\sim \\\\text{{Normal}}(\\\\mu = {mu}, \\\\sigma = {sigma})$.\"\ncntxt_b = \"  The probability density function for the Normal random variable is \\\\[ f(x | \\\\mu, \\\\sigma) = (2\\\\pi\\\\sigma^2)^{-1/2} e^{{\\\\frac{{-(x - \\\\mu)^2}}{{2\\\\sigma^2}} }}\\\\] for $x \\\\in (-\\\\infty, \\\\infty)$ and $\\\\mu \\\\in (-\\\\infty, \\\\infty)$ and $\\\\sigma > 0$.\"\ncntxt = cntxt_a + cntxt_b\n\nq1 = \"By hand, calculate the simplified log-likelihood and then write the simplified log-likelihood as a Python function with signature $\\\\texttt{ll_normal(theta, x)}$.\"\nq2 = \"By  hand, calculate the maximum likelihood estimator.  Include a photo of your hand written solution, or type out your solution using LaTeX.\"\nq3 = f\"Create an array $\\\\texttt{{X}}$ by generating $N = 1001$ Normal random variables, with $\\\\mu = {mu}$ and $\\\\sigma = {sigma}$ from above.  Use $\\\\texttt{{np.random.normal(}}\\\\mu, \\\\sigma \\\\texttt{{, N)}}$.\"\nq4 = \"Use $\\\\texttt{scipy.optimize.minimize(ll, theta0, args=(X), method='L-BFGS-B', bounds=[...])}$ to calculate the maximum likelihood estimate $\\\\hat{\\\\theta}$ of $\\\\theta = (\\\\mu, \\\\sigma)$.\"\n\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'context': cntxt,\n                     'questions': [q1, q2, q3, q4],\n                     'random': {\"mu\": float(mu), \"sigma\": float(sigma)}})\n\nprint(output)\n#< /exercise >#\n#< #solution >#\n\nnp.random.seed(seed)\nX = np.random.normal(mu, sigma, 1001)\n\ndef ll_normal(theta, x):\n    N = x.size\n    mu = theta[0]\n    sigma = theta[1]\n    return N * np.log(sigma) + np.sum((x - mu)**2) / (2 * sigma**2)\n\nsol = np.round(minimize(ll_normal, (1, 1), args=(X), method=\"L-BFGS-B\", bounds=[(-np.inf, np.inf), (1e-5, np.inf)])[\"x\"], 2)\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'random': {\"mu\": float(mu), \"sigma\": float(sigma)},\n                     'solutions': sol.tolist()})\nprint(output)\n#< /solution >#\n",
      "tags": ["likelihood", "normal", "simulate", "python"]
    },
    {
      "id": "2RsT",
      "language": "python",
      "exercise": "import json\nimport numpy as np\nfrom scipy import stats\nfrom scipy.optimize import minimize\n\nseed = #< SEED >#\nid = '#< ID >#'\n\nnpint = np.iinfo(np.int)\nif seed is None:\n    seed = np.random.randint(1, npint.max)\nnp.random.seed(seed)\n\nl = np.round(np.random.uniform(1, 15), 2)\n\n\n#< #exercise >#\ncntxt_a = f\"Let $X_1, \\\\ldots, X_N \\\\sim \\\\text{{Poisson}}(\\\\lambda = {l})$.\"\ncntxt_b = \"  The probability density function for the Poisson random variable is \\\\[ f(x | \\\\lambda) = \\\\lambda^x e^{{-\\\\lambda}} / x! \\\\] for $x \\\\in 0, 1, 2, 3, \\\\ldots$ and $\\\\lambda > 0$\"\ncntxt = cntxt_a + cntxt_b\n\nq1 = \"By hand, calculate the simplified log-likelihood and then write the simplified log-likelihood as a Python function with signature $\\\\texttt{ll_poisson(l, x)}$.\"\nq2 = \"By  hand, calculate the maximum likelihood estimator.  Include a photo of your hand written solution, or type out your solution using LaTeX.\"\nq3 = f\"Create an array $\\\\texttt{{X}}$ by generating $N = 1001$ Poisson random variables, with $\\\\lambda = {l}$ from above.  Use $\\\\texttt{{np.random.poisson(}}\\\\lambda \\\\texttt{{, N)}}$.\"\nq4 = \"Use $\\\\texttt{scipy.optimize.minimize(ll, (l0), args=(X), method='L-BFGS-B', bounds=[...])}$ to calculate the maximum likelihood estimate $\\\\hat{\\\\lambda}$ of $\\\\lambda$.\"\n\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'context': cntxt,\n                     'questions': [q1, q2, q3, q4],\n                     'random': {\"lambda\": float(l)}})\n\nprint(output)\n#< /exercise >#\n#< #solution >#\n\nnp.random.seed(seed)\nX = np.random.poisson(l, 1001)\n\ndef ll_poisson(l, x):\n    N = x.size\n    return -np.log(l) * np.sum(X) + N*l\n\n\nsol = np.round(minimize(ll_poisson, (1), args=(X), method=\"L-BFGS-B\", bounds=[(1e-5, np.inf)])[\"x\"], 2)\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'random': {\"lambda\": float(l)},\n                     'solutions': float(sol)})\nprint(output)\n#< /solution >#\n",
      "tags": ["likelihood", "poisson", "simulate", "python"]
    },
    {
      "id": "oOAf",
      "language": "python",
      "exercise": "import json\nimport numpy as np\nfrom scipy import stats\nfrom scipy.optimize import minimize\n\nseed = #< SEED >#\nid = '#< ID >#'\n\nnpint = np.iinfo(np.int)\nif seed is None:\n    seed = np.random.randint(1, npint.max)\nnp.random.seed(seed)\n\nm = np.round(np.random.uniform(1, 15), 2)\nl = np.round(np.random.uniform(1, 15), 2)\n\n\n#< #exercise >#\ncntxt_a = f\"Let $X_1, \\\\ldots, X_N \\\\sim \\\\text{{IG}}(\\\\mu = {m}, \\\\lambda = {l})$, named inverse Gaussian or Wald.\"\ncntxt_b = \"  The probability density function for the inverse Gaussian random variable is \\\\[ f(x | \\\\mu, \\\\lambda) = \\\\left( \\\\frac{{\\\\lambda}}{{2\\\\pi x^3}} \\\\right)^{1/2} e^{{ -\\\\frac{{\\\\lambda(x - \\\\mu)^2}}{{2\\\\mu^2x}} }} \\\\] for $x > 0$, $\\\\mu > 0$, and $\\\\lambda > 0$.\"\ncntxt = cntxt_a + cntxt_b\n\nq1 = \"By hand, calculate the simplified log-likelihood and then write the simplified log-likelihood as a Python function with signature $\\\\texttt{ll_ig(theta, x)}$.\"\nq2 = f\"Create an array $\\\\texttt{{X}}$ by generating $N = 1001$ IG/Wald random variables, with $\\\\mu = {m}$, $\\\\lambda = {l}$ from above.  Use $\\\\texttt{{np.random.wald(}}\\\\mu, \\\\lambda \\\\texttt{{, N)}}$.\"\nq3 = \"Use $\\\\texttt{scipy.optimize.minimize(ll, theta0, args=(X), method='L-BFGS-B', bounds=[...])}$ to calculate the maximum likelihood estimate $\\\\hat{\\\\theta}$ of $\\\\theta = (\\\\mu, \\\\lambda)$.\"\n\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'context': cntxt,\n                     'questions': [q1, q2, q3],\n                     'random': {\"m\": float(m), \"lambda\": float(l)}})\n\nprint(output)\n#< /exercise >#\n#< #solution >#\n\nnp.random.seed(seed)\nX = np.random.wald(m, l, 1001)\n\ndef ll_ig(theta, x):\n  m = theta[0]\n  l = theta[1]\n  N = x.size\n  one = -0.5 * N * np.log(l) + 1.5 * np.sum(np.log(x))\n  two = (l / (2*m**2)) * np.sum((x - m)**2 / x)\n  return one + two\n\n\nsol = np.round(minimize(ll_ig, (1, 1), args=(X), method=\"L-BFGS-B\", bounds=[(1e-5, np.inf), (1e-5, np.inf)])[\"x\"], 2)\n\noutput = json.dumps({'seed': seed,\n                     'id': id,\n                     'random': {\"mu\": float(m), \"lambda\": float(l)},\n                     'solutions': sol.tolist()})\nprint(output)\n#< /solution >#\n",
      "tags": ["likelihood", "inverse gaussian", "wald", "simulate", "python"]
    }
  ],
  "last_updated": "Fri Sep 27 2019 11:17:56 GMT-0700 (Pacific Daylight Time)"
}
